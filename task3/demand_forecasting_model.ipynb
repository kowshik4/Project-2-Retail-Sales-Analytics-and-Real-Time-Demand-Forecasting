{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UlciKMHQaPkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6caebe-e46b-42e4-f0a6-dc8d4a364a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.feature import StandardScaler\n",
        "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "\n",
        "# Set up Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Retail Demand Forecasting\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from CSV file\n",
        "file_path = \"/content/Online_Retail.csv\"  # Update with your file path\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the schema to verify correct loading\n",
        "df.printSchema()\n",
        "\n",
        "# Display a sample of the dataset\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUdQjX1jzo_m",
        "outputId": "c750b2d6-eacc-4432-ec10-c7b6888a1a72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set legacy time parser policy\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "# Convert InvoiceDate to timestamp\n",
        "df = df.withColumn(\"InvoiceDate\", to_timestamp(\"InvoiceDate\", \"MM/dd/yyyy HH:mm\"))\n",
        "\n",
        "# Create a new column for date (excluding time) for aggregation\n",
        "df = df.withColumn(\"Date\", to_date(\"InvoiceDate\"))\n",
        "\n",
        "# Filter out rows with missing or invalid values\n",
        "df = df.na.drop(subset=[\"StockCode\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\"])\n",
        "\n",
        "# Remove rows with negative or zero Quantity\n",
        "df = df.filter(df.Quantity > 0)\n",
        "\n",
        "# Display preprocessed data\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "id": "lIMRcEGb1veI",
        "outputId": "479a68fc-0f87-4a62-eafa-bfadad5e6601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|      Date|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|2010-12-01|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|2010-12-01|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|2010-12-01|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|2010-12-01|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|2010-12-01|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate data at StockCode and Date level\n",
        "agg_data = df.groupBy(\"StockCode\", \"Date\") \\\n",
        "    .agg(sum(\"Quantity\").alias(\"TotalQuantity\")) \\\n",
        "    .orderBy(\"Date\")\n",
        "\n",
        "# Display the aggregated data\n",
        "agg_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOVYT7vO6fER",
        "outputId": "d06b6546-ea34-43a3-9665-738e6f94fcbc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-------------+\n",
            "|StockCode|      Date|TotalQuantity|\n",
            "+---------+----------+-------------+\n",
            "|    21559|2010-12-01|            8|\n",
            "|   84029E|2010-12-01|          551|\n",
            "|   35004C|2010-12-01|          174|\n",
            "|    22086|2010-12-01|          274|\n",
            "|    21210|2010-12-01|           13|\n",
            "+---------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add lag features using a window function\n",
        "window_spec = Window.partitionBy(\"StockCode\").orderBy(\"Date\")\n",
        "agg_data = agg_data.withColumn(\"Lag_1\", lag(\"TotalQuantity\", 1).over(window_spec)) \\\n",
        "                   .withColumn(\"Lag_2\", lag(\"TotalQuantity\", 2).over(window_spec)) \\\n",
        "                   .withColumn(\"Lag_3\", lag(\"TotalQuantity\", 3).over(window_spec))\n",
        "\n",
        "# Drop rows with null values created by lag features\n",
        "agg_data = agg_data.na.drop()\n",
        "\n",
        "# Display the data\n",
        "agg_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWyS5iJt7FAw",
        "outputId": "c6fa5b08-d329-4ce1-aa0d-f0b8b64e1afe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-------------+-----+-----+-----+\n",
            "|StockCode|      Date|TotalQuantity|Lag_1|Lag_2|Lag_3|\n",
            "+---------+----------+-------------+-----+-----+-----+\n",
            "|    10002|2010-12-13|           27|   48|   44|   13|\n",
            "|    10002|2010-12-14|            7|   27|   48|   44|\n",
            "|    10002|2010-12-16|            5|    7|   27|   48|\n",
            "|    10002|2010-12-17|            2|    5|    7|   27|\n",
            "|    10002|2010-12-20|            2|    2|    5|    7|\n",
            "+---------+----------+-------------+-----+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble features into a single vector\n",
        "feature_cols = [\"Lag_1\", \"Lag_2\", \"Lag_3\"]\n",
        "agg_data = agg_data.withColumn(\"features\", array(*feature_cols))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = agg_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Display the sizes of the training and testing datasets\n",
        "print(f\"Training Data Count: {train_data.count()}\")\n",
        "print(f\"Testing Data Count: {test_data.count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izhZUPaJ7MIB",
        "outputId": "29a5e940-4b44-4ff4-9407-8d391371d82b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Count: 196142\n",
            "Testing Data Count: 49053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to RDD for scaling\n",
        "train_rdd = train_data.rdd.map(lambda row: LabeledPoint(row[\"TotalQuantity\"], Vectors.dense(row[\"features\"])))\n",
        "test_rdd = test_data.rdd.map(lambda row: LabeledPoint(row[\"TotalQuantity\"], Vectors.dense(row[\"features\"])))\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler(withMean=True, withStd=True).fit(train_rdd.map(lambda x: x.features))\n",
        "train_rdd_scaled = train_rdd.map(lambda lp: LabeledPoint(lp.label, scaler.transform(lp.features)))\n",
        "test_rdd_scaled = test_rdd.map(lambda lp: LabeledPoint(lp.label, scaler.transform(lp.features)))\n"
      ],
      "metadata": {
        "id": "QoYiYUEP-uBQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Assemble features into a single vector (convert array to VectorUDT)\n",
        "assembler = VectorAssembler(inputCols=[\"Lag_1\", \"Lag_2\", \"Lag_3\"], outputCol=\"features_vector\")\n",
        "train_data_ml = assembler.transform(train_data).select(col(\"TotalQuantity\").alias(\"label\"), col(\"features_vector\").alias(\"features\"))\n",
        "test_data_ml = assembler.transform(test_data).select(col(\"TotalQuantity\").alias(\"label\"), col(\"features_vector\").alias(\"features\"))\n",
        "\n",
        "# Define the Linear Regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Train the model\n",
        "lr_model = lr.fit(train_data_ml)\n",
        "\n",
        "# Display model coefficients and intercept\n",
        "print(f\"Coefficients: {lr_model.coefficients}\")\n",
        "print(f\"Intercept: {lr_model.intercept}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCEokjMRBhfZ",
        "outputId": "629b6408-04b5-422f-c720-3dc8fae23818"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.09469991208579596,0.10811459367712797,0.11838071215482396]\n",
            "Intercept: 13.80930616836672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = lr_model.transform(test_data_ml)\n",
        "\n",
        "# Evaluate the model using RMSE and MAE\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator_rmse.evaluate(predictions)\n",
        "\n",
        "evaluator_mae = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator_mae.evaluate(predictions)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# Display sample predictions\n",
        "predictions.select(\"features\", \"label\", \"prediction\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v281jBb6YGUU",
        "outputId": "f25e412a-fec1-440f-f4ef-012b81212f95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 62.97587733077631\n",
            "Mean Absolute Error (MAE): 20.19762219441922\n",
            "+---------------+-----+------------------+\n",
            "|       features|label|        prediction|\n",
            "+---------------+-----+------------------+\n",
            "|[7.0,27.0,48.0]|    5|23.073573765681296|\n",
            "| [12.0,2.0,2.0]|   60|15.398695725060175|\n",
            "|[1.0,60.0,12.0]|   24|21.811450246938083|\n",
            "| [24.0,1.0,7.0]|   13| 17.01888363718672|\n",
            "| [14.0,2.0,1.0]|  132|15.469714837076944|\n",
            "+---------------+-----+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert features to a string format for saving\n",
        "predictions_to_save = predictions.withColumn(\"features\", col(\"features\").cast(\"string\"))\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "output_path = \"/content/predictions.csv\"\n",
        "predictions_to_save.select(\"features\", \"label\", \"prediction\").write.csv(output_path, header=True)\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSAR26ahdx0f",
        "outputId": "7171fa66-5018-4a28-f713-e429a9397031"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/predictions.csv\n"
          ]
        }
      ]
    }
  ]
}