{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b54cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840634 sha256=aadd05d63b83400aca798711633ba047bafcf8be2e418e6b2ae82870f8fc638f\n",
      "  Stored in directory: /Users/kowshikmosalakanti/Library/Caches/pip/wheels/2e/d2/18/6f4f20e8332359f7fffceb6828edcc80ef96f86744192a7bb9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2ddfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, lit, udf, explode\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d590001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 20:01:11 WARN Utils: Your hostname, Kowshiks-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.20.10.105 instead (on interface en0)\n",
      "24/11/30 20:01:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/30 20:01:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Sentiment Analysis\").getOrCreate()\n",
    "\n",
    "# Load Dataset\n",
    "file_path = \"amazon_reviews.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd88735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Keep necessary columns and drop rows with null values\n",
    "    df = df.select(\"reviewText\", \"overall\").na.drop()\n",
    "\n",
    "    # Labeling sentiment based on overall rating (Positive: >=4, Negative: <4)\n",
    "    df = df.withColumn(\"label\", when(col(\"overall\") >= 4, 1).otherwise(0))\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "\n",
    "    # Stop Words Removal\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "    # Feature Extraction (TF-IDF)\n",
    "    count_vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "    # Assemble stages\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, count_vectorizer, idf])\n",
    "    model = pipeline.fit(df)\n",
    "    return model.transform(df)\n",
    "\n",
    "df_preprocessed = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f03688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 20:01:57 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/11/30 20:01:57 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train, test = df_preprocessed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Sentiment Classification Model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a395a0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "predictions = model.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Sentiment Scoring by Product\n",
    "# Aggregate sentiment scores for each product\n",
    "sentiment_udf = udf(lambda x: \"Positive\" if x == 1 else \"Negative\", StringType())\n",
    "df_with_sentiment = predictions.withColumn(\"sentiment\", sentiment_udf(col(\"prediction\")))\n",
    "\n",
    "df_sentiment_scores = (\n",
    "    df_with_sentiment.groupBy(\"overall\")\n",
    "    .agg(expr(\"AVG(prediction) AS avg_sentiment_score\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4b533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "output_path = \"path_to_output.csv\"\n",
    "df_sentiment_scores.write.csv(output_path, header=True)\n",
    "\n",
    "print(\"Sentiment analysis completed and results saved to output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4bb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
