{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: pyspark in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#pyspark setup\n",
    "\n",
    "!apt-get update\n",
    "# Install Java 8 (required by Spark)\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Install Spark\n",
    "!pip install pyspark\n",
    "\n",
    "# setup environment variables\n",
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.10/dist-packages/pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 03:56:02 WARN Utils: Your hostname, codespaces-3e6bbc resolves to a loopback address: 127.0.0.1; using 10.0.2.121 instead (on interface eth0)\n",
      "24/12/06 03:56:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/06 03:56:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 03:56:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Retail Sales Analytics\").getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "sales_df = spark.read.csv(\"/workspaces/Project-2-Retail-Sales-Analytics-and-Real-Time-Demand-Forecasting/task1/Online Retail - Online Retail (1).csv\", header=True, inferSchema= True) # Corrected the filename\n",
    "reviews_df = spark.read.csv(\"/workspaces/Project-2-Retail-Sales-Analytics-and-Real-Time-Demand-Forecasting/task1/reviews.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Explore data\n",
    "sales_df.show(10)\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Check schema\n",
    "sales_df.printSchema()\n",
    "reviews_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Sales Data:\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Missing Values in Reviews Data:\n",
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Missing Values in Sales Data:\")\n",
    "sales_df.select([col(column).isNull().alias(column) for column in sales_df.columns]).show()\n",
    "\n",
    "print(\"Missing Values in Reviews Data:\")\n",
    "reviews_df.select([col(column).isNull().alias(column) for column in reviews_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 03:56:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------------------+--------+-----------+---------+------------------+-------+\n",
      "|summary|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|        CustomerID|Country|\n",
      "+-------+---------+---------+--------------------+--------+-----------+---------+------------------+-------+\n",
      "|  count|   541909|   541909|              541909|  541909|     541909|   541909|            541909| 541909|\n",
      "|   mean|      0.0|      0.0|0.002683107311375157|     0.0|        0.0|      0.0| 0.249266943342886|    0.0|\n",
      "| stddev|      0.0|      0.0|0.051729229498118534|     0.0|        0.0|      0.0|0.4325890424198415|    0.0|\n",
      "|    min|        0|        0|                   0|       0|          0|        0|                 0|      0|\n",
      "|    25%|        0|        0|                   0|       0|          0|        0|                 0|      0|\n",
      "|    50%|        0|        0|                   0|       0|          0|        0|                 0|      0|\n",
      "|    75%|        0|        0|                   0|       0|          0|        0|                 0|      0|\n",
      "|    max|        0|        0|                   1|       0|          0|        0|                 1|      0|\n",
      "+-------+---------+---------+--------------------+--------+-----------+---------+------------------+-------+\n",
      "\n",
      "+-------+------+---------+\n",
      "|summary|Review|Sentiment|\n",
      "+-------+------+---------+\n",
      "|  count|   386|      386|\n",
      "|   mean|   0.0|      0.0|\n",
      "| stddev|   0.0|      0.0|\n",
      "|    min|     0|        0|\n",
      "|    25%|     0|        0|\n",
      "|    50%|     0|        0|\n",
      "|    75%|     0|        0|\n",
      "|    max|     0|        0|\n",
      "+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select([(col(c).isNull().cast(\"int\")).alias(c) for c in sales_df.columns]).summary().show()\n",
    "reviews_df.select([(col(c).isNull().cast(\"int\")).alias(c) for c in reviews_df.columns]).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 03:56:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset:\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n",
      "Handling missing values...\n",
      "Dataset after dropping missing values:\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Filtering non-meaningful text...\n",
      "Dataset after filtering non-meaningful text:\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Validating sentiment values...\n",
      "Dataset after validating sentiment:\n",
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "+------+---------+\n",
      "\n",
      "Dropping duplicates...\n",
      "Final cleaned dataset:\n",
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "+------+---------+\n",
      "\n",
      "Cleaned dataset saved to cleaned_reviews_data.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Data Cleansing - Simplified Reviews\").getOrCreate()\n",
    "\n",
    "# Load the review dataset\n",
    "reviews_data_path = \"/workspaces/Project-2-Retail-Sales-Analytics-and-Real-Time-Demand-Forecasting/task1/reviews.csv\"  # Replace with the actual file path\n",
    "reviews_df = spark.read.csv(reviews_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect data\n",
    "print(\"Initial dataset:\")\n",
    "reviews_df.show(10)\n",
    "reviews_df.printSchema()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "reviews_df = reviews_df.dropna(subset=[\"Review\", \"Sentiment\"])\n",
    "\n",
    "# Debugging step: Check dataset after dropping missing values\n",
    "print(\"Dataset after dropping missing values:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Remove rows with empty or whitespace-only reviewText\n",
    "print(\"Filtering non-meaningful text...\")\n",
    "reviews_df = reviews_df.filter(col(\"Review\").rlike(r\"\\S+\"))\n",
    "\n",
    "# Debugging step: Check dataset after filtering non-meaningful text\n",
    "print(\"Dataset after filtering non-meaningful text:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Validate sentiment values (should be \"positive\" or \"negative\")\n",
    "print(\"Validating sentiment values...\")\n",
    "valid_sentiments = [\"positive\", \"negative\"]\n",
    "reviews_df = reviews_df.filter(col(\"Sentiment\").isin(valid_sentiments))\n",
    "\n",
    "# Debugging step: Check dataset after validating sentiment\n",
    "print(\"Dataset after validating sentiment:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Drop duplicates\n",
    "print(\"Dropping duplicates...\")\n",
    "reviews_df = reviews_df.dropDuplicates()\n",
    "\n",
    "# Final dataset preview\n",
    "print(\"Final cleaned dataset:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Save cleaned data for further use\n",
    "output_path = \"cleaned_reviews_data.csv\"  # Replace with your desired output path\n",
    "reviews_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "print(f\"Cleaned dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 03:56:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536375|   84406B|CREAM CUPID HEART...|       8| 2010-01-12|     2.75|     17850|United Kingdom|\n",
      "|   536389|   85014A|BLACK/BLUE POLKAD...|       3| 2010-01-12|     5.95|     12431|     Australia|\n",
      "|   536408|    22492|MINI PAINT SET VI...|      36| 2010-01-12|     0.65|     14307|United Kingdom|\n",
      "|   536425|    22829|SWEETHEART WIRE W...|       2| 2010-01-12|     9.95|     13758|United Kingdom|\n",
      "|   536437|    22189|CREAM HEART CARD ...|      72| 2010-01-12|     3.39|     13694|United Kingdom|\n",
      "|   536528|   85114C|RED ENCHANTED FOR...|       1| 2010-01-12|     1.65|     15525|United Kingdom|\n",
      "|   536532|    22555|PLASTERS IN TIN S...|      36| 2010-01-12|     1.65|     12433|        Norway|\n",
      "|   536532|    21786|  POLKADOT RAIN HAT |      24| 2010-01-12|     0.42|     12433|        Norway|\n",
      "|   536562|   79302M|ART LIGHTS,FUNK M...|       6| 2010-01-12|     2.95|     13468|United Kingdom|\n",
      "|   536569|    22825|DECORATIVE PLANT ...|       1| 2010-01-12|     7.95|     16274|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Retail Sales Analytics\").getOrCreate()\n",
    "\n",
    "# Set legacy time parser policy to handle the problematic date format\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Load datasets\n",
    "sales_df = spark.read.csv(\"/workspaces/Project-2-Retail-Sales-Analytics-and-Real-Time-Demand-Forecasting/task1/Online Retail - Online Retail (1).csv\", header=True, inferSchema= True) # Corrected the filename\n",
    "\n",
    "\n",
    "# Inspect data\n",
    "sales_df.show(10)\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Handle missing values\n",
    "# Drop rows with null CustomerID or Description, as they are critical fields\n",
    "sales_df = sales_df.dropna(subset=[\"StockCode\", \"Description\"])\n",
    "\n",
    "# Drop duplicates\n",
    "sales_df = sales_df.dropDuplicates()\n",
    "\n",
    "# Parse InvoiceDate to a proper date format\n",
    "# Use the correct format based on your data (e.g., \"dd/MM/yy HH:mm\")\n",
    "sales_df = sales_df.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"dd/MM/yy HH:mm\"))\n",
    "\n",
    "# Filter out invalid data (e.g., Quantity or UnitPrice less than or equal to 0)\n",
    "sales_df = sales_df.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
    "\n",
    "# Final schema and data preview\n",
    "sales_df.printSchema()\n",
    "sales_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets for downstream tasks\n",
    "sales_df.write.csv(\"cleaned_sales_data.csv\", header=True, mode=\"overwrite\")\n",
    "reviews_df.write.csv(\"cleaned_reviews_data.csv\", header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
